# Login to the cluster:

ssh ntavakoli6@login-phoenix.pace.gatech.edu 
ssh ntavakoli6@login-phoenix-3.pace.gatech.edu


# To submit an interactive job:
 qsub -I -q inferno -A GT-saluru8-CODA20 -l nodes=1:ppn=24,mem=100gb,walltime=96:00:00

module load anaconda3 gurobi
export PYTHONPATH=$GUROBI_HOME/lib/python3.8_utf32:$PYTHONPATH

cd /storage/coda1/p-saluru8/0/ntavakoli6/

 $ To submit batch

#PBS -N ED_ILP_150_8_10k
#PBS -j oe
#PBS -o ED_ILPlog_150_8_10k.out
#PBS -m abe                            
#PBS -A GT-saluru8-CODA20          # account to which job is charged, ex: GT-gburdell3
#PBS -l nodes=1:ppn=1              # number of nodes and cores per node required
#PBS -l pmem=700gb                 # memory per core
#PBS -l walltime=96:00:00

# change according to the job submission requirement of your cluster


project_dir=/storage/coda1/p-saluru8/0/ntavakoli6/hged

cd ${project_dir}
bcftools=/storage/coda1/p-saluru8/0/ntavakoli6/software/bcftools/bcftools

cd ${DATA}

module load anaconda3 gurobi
export PYTHONPATH=$GUROBI_HOME/lib/python3.8_utf32:$PYTHONPATH

echo "start running"
python main.py ../data/graph_alpha_150_subrange_10000/chr22_vg_edges_alpha_150_subrange_10000.txt\
    ../data/graph_alpha_150_subrange_10000/chr22_POS_substrings_len_150_subrange_10000.txt 150 8 

